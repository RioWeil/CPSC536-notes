\section{Adversary Method}
\textcolor{red}{Missed this lecture - notes based on provided references.}

The quantum adversary method provides a way to prove quantum query lower bounds. In fact, we will find that the generalized version is an upper bound on quantum query complexity, up to constant factors.

\subsection*{Quantum adversaries}
Motivation for the quantum adversary method comes from the following construction. Suppose the oracle is operated by an adversarial party who holds a quantum state determining the oracle string, which is in some superposition $\sum_{x \in S}\ket{a_x}\ket{x}$ over the possible oracles. To implement eac query, the adversary performs the ``super-oracle'':
\begin{equation}
    O \coloneqq \sum_{x \in S}\dyad{x}{x} \otimes O_x.
\end{equation}
An algorithm does not have direct access to the oracle string, and hence can only perform unitary operations that act as the identity on the adversary's superposition. After $t$ steps, an algorithm maps the overall state to:
\begin{equation}
    \ket{\psi^t} \coloneqq (I \otimes U_t)O\ldots (I \otimes U_2)O(I \otimes U_1)O\left(\sum_{x \in S}a_x \ket{x} \otimes \ket{\psi}\right) = \sum_{x \in S}a_x\ket{x} \otimes \ket{\psi^t_x}.
\end{equation}
The main idea of the approach is that for the algorithm to learn $x$, this state must become very entangled. To measure the entanglement of the pure state $\ket{\psi^t}$, we can consider the reduced density matrix of the oracle,
\begin{equation}
    \rho^t \coloneqq \sum_{x, y \in S}a_x^* a_y\braket{\psi_x^t}{\psi_y^t}\dyad{x}{y}
\end{equation}
Initially, the state $\rho^0$ is pure. Our goal is to quantify how mixed it must become (i.e. how entangled the overall state must be) before we can compute $f$ with error at most $\e$. To do this we could consider, for example, the entropy of $\rho^t$. However, it turns out that other measures are easier to deal with.

In particular, we have the following fact about the distinguishability of quantum states

\begin{factbox}{}
    Given one of pure states $\ket{\psi}, \ket{\phi}$, we can make a measurement that determines which state we have with error probability at most $\e \in [0, 1/2]$ if and only if $\abs{\braket{\psi}{\phi}} \leq 2\sqrt{\e(1-\e)}$.
\end{factbox}

Thus, it is convenient to consider measures that are linear in the inner products $\braket{\psi_x^t}{\psi_y^t}$. 

\subsection*{The adversary method}
To obtain an adversary lower bound, we choose a matrix $\Gamma \in \RR^{\abs{S} \times \abs{S}}$ with rows and columns indexed by the possible black-box inputs. The entry $\Gamma_{x, y}$ is meant to characterize how hard it is to distinguish between $x$ and $y$. We say that $\Gamma$ is an \emph{adversary matrix} if:
\begin{enumerate}
    \item $\Gamma_{xy} = \Gamma_{yx}$
    \item If $f(x) = f(y)$ then $\Gamma_{xy} = 0$.
\end{enumerate}
The second condition reflects that we do not need to distinguish between $x$ and $y$ if $f(x) = f(y)$.

The original adversary method made the additional assumption that $\Gamma_{xy} \geq 0$ but it turns out this condition is not actually unecessary. Sometimes we refer to the \emph{negative} or \emph{generalized} adversary method to distinguish it from the original, positive-weighted method. While it may not be intuitively obvious what it would mean to give a negative weight to the entry characterizing distinguishability of two inputs, it turns out that this flexibility can lead to significantly improved lower bounds for some functions..

Given an adversary matrix $\Gamma$, we can define a weight function:

\begin{equation}
    W^j \coloneqq \sum_{x, y \in S}\Gamma_{xy}a^*_x a_y\braket{\psi^j_x}{\psi^j_y}.
\end{equation}

Note that this is a simple function of the entries of $\rho^j$. The idea of the lower bound is to show that $W^j$ starts out large, must become small to compute $f$, and cannot change by much if we make a query.

the initial value of the weight function is:
\begin{equation}
    W^0 = \sum_{x, y \in S}\Gamma_{xy}a^*_x a_y\braket{\psi^0_x}{\psi^0_y} = \sum_{x, y \in S}a_x^* \Gamma_{xy}a_y
\end{equation}
since $\ket{\psi^0_x}$ cannot depend on $x$. To make this as large as possible, we take $a$ to be a principal eignevector of $\Gamma$, an eigenvector with eigenvalue $\pm \norm{\Gamma}$. Then $\abs{W^0} = \norm{\Gamma}$. 

The final value of the weight function is easier to bound if we assume a nonnegative adversary matrix. The final value is constrained by the fact that we must distinguish $x$ from $y$ with error probability at most $\e$ whenever $f(x) \neq f(y)$. For this to hold after $t$ queries, we need $\abs{\braket{\psi^t_x}{\psi^t_y}} \leq 2\sqrt{\e(1-\e)}$ for all pairs $x, y \in S$ with $f(x) \neq f(y)$ (by the above fact). Thus, if $\Gamma$ has nonnegative entries, we have:
\begin{equation}
    \abs{W^t} \leq \sum{x, y \in S}\Gamma_{xy}a_x^*a_y2\sqrt{\e(1-\e)} = 2\sqrt{\e(1-\e)}\norm{\Gamma}.
\end{equation}
Here we can include the terms where $f(x) = f(y)$ in the sum since $\Gamma_{xy} = 0$ for such pairs. We also used the fact that the principal eigenvector of a nonnegative matrix can be taken to have nonnegative entries (by the Perron-Frobenius theorem).


A similar bound holds if $\Gamma$ has negative entries, but we need a different argument. In general, one can only show that $\abs{W^t} \leq (2\sqrt{\e(1-\e)} + 2\e)\norm{\Gamma}$. But if we assume $f: S \rightarrow \set{0, 1}$ has Boolean output, then we can prove the same bound as in the non-negative cae, and the proof is simpler than for a general output space. We can use the following simple result, stated in terms of the Frobenius norm $\norm{X}^2_F \coloneqq \sum_{a, b}\abs{X_{ab}}^2$:

\begin{propbox}{}
    For any $X \in \CC^{m \times n}$, $Y \in \CC^{n \times n}$, $Z \in \CC^{n \times m}$, we have $\abs{\Tr(XYZ)} \leq \norm{X}_F\norm{Y}\norm{Z_F}$.
\end{propbox}
\begin{proof}
    We have:
    \begin{equation}
        \Tr(XYZ) = \sum_{a, b, c}X_{ab}Y_{bc}Z_{ca} = \sum_a(x^a)^\dag Y z^a
    \end{equation}
    where $(x^a)_b = X^*_{ab}$ and $(z^a)_c = Z_{ca}$. Thus:
    \begin{align*}
        \abs{\Tr(XYZ)} &\leq \sum_a \norm{x^a}\norm{Yz^a}
        \\ &\leq \norm{Y}\sum_a \norm{x^a}\norm{z^a}
        \\ &\leq \norm{Y}\sqrt{\sum_a \norm{x^a}^2\sum_{a'}\norm{z^{a'}}^2}
        \\ &= \norm{Y}\norm{X}_F\norm{Z}_F
    \end{align*}
    as claimed, where we use Cauchy-Shwarz in the first and third steps.
\end{proof}

The upper bound $\abs{W^t}$ for the negative adversary with Boolean output, write $W^t = \Tr(\Gamma V)$ where $V_{xy} \coloneqq a_x^*a_y\braket{\psi^t_x}{\psi^t_y}\delta[f(x) \neq f(y)]$. Now define:
\begin{equation}
    C \coloneqq \sum_{x \in S}a_x\Pi_{f(x)}\dyad{\psi^t_x}{x}
\end{equation}
\begin{equation}
    \overline{C} \coloneqq \sum_{x \in S}a_x\Pi_{1-f(x)}\dyad{\psi^t_x}{x}
\end{equation}
with $\Pi_0, \Pi_1$ projectors onto the $f(x)=0,1$ respectively. Then:
\begin{equation}
    (C^\dag\overline{C})_{xy} = a_x^*a_y\bra{\psi^t_x}\Pi_{f(x)}\Pi_{1-f(y)}\ket{\psi^t_y},
\end{equation}
so:
\begin{equation}
    (C^\dag \overline{C} + \overline{C}^\dag C)_{xy} = a_x^*a_y\bra{\psi_x^t}(\Pi_{f(x)}\Pi_{1-f(y)} + \Pi_{1-f(x)}\Pi_{f(y)})\ket{\psi^t_y} = a_x^*a_y\braket{\psi_x^t}{\psi_y^t}\delta[f(x) \neq f(y)],
\end{equation}
i.e. $V = C^\dag \overline{C} + \overline{C}^\dag C$. Thus we have:
\begin{equation}
    W^t = \Tr(\Gamma(C^\dag\overline{C} + \overline{C}^\dag C)) = \Tr(\overline{C}\Gamma C^\dag) + \Tr(C\Gamma\overline{C}^\dag).
\end{equation}
By the proposition, $\abs{W^t} \leq 2\norm{\Gamma}\norm{C}_F\norm{\overline{C}}_F$. Finally, we upper bound $\norm{C}_F, \norm{\overline{C}}_F$:
\begin{equation}
    \norm{C}_F^2 + \norm{\overline{C}}_F^2 = \sum_{x, y \in S}\abs{a_x}^2(\abs{\bra{y}\Pi_{f(x)}\ket{\psi^t_x}}^2 + \abs{\bra{y}\Pi_{1-f(x)}\ket{\psi^t_x}}^2)=1
\end{equation}
\begin{equation}
    \norm{\overline{C}}_F^2 = \sum_{x \in S}\abs{a_x}^2\norm{\Pi_{1-f(x)}\ket{\psi_x^t}}^2 \leq \e
\end{equation}
Therefore $\norm{C}_F\norm{\overline{C}}_F \leq \max_{x \in [0, \e]}\sqrt{x(1-x)} = \sqrt{\e(1-\e)}$ assuming $\e \in [0, 1/2]$ and thus $\abs{W^t} \leq 2\sqrt{\e(1-\e)}\norm{\Gamma}$ as claimed.

It remains to understand how much the weight function can decrease at each step of the algorithm. We have:
\begin{equation}
    W^{j+1} - W^j = \sum_{x, y \in S} \Gamma_{xy}a_x^* a_y(\braket{\psi_x^{j+1}}{\psi_y^{j+1}} - \braket{\psi_x^j}{\psi_y^j})
\end{equation}
Consider how the state changes when we make a query. We have $\ket{\psi_x^{j+1}} = U_{j+1}O_x\ket{\psi_x^j}$. Thus the elements of the Gram matrix of the states $\set{\ket{\psi_x^{j+1}}: x \in S}$ are:
\begin{equation}
    \braket{\psi_x^{j+1}}{\psi_y^{j+1}} = \bra{\psi_x^j}O_x^\dag (U_{j+1})^\dag U_{j+1}O_y\ket{\psi_y^j} = \bra{\psi_x^j}O_xO_y\ket{\psi_y^j}
\end{equation}
since $U_{j+1}$ is unitary and $O_x$ is Hermitian. Therefore:
\begin{equation}
    W^{j+1} - W^j = \sum_{x, y \in S} \Gamma_{xy}a_x^*a_Y\bra{\psi^j_x}(O_xO_y - I)\ket{\psi^j_y}.
\end{equation}
Observe that $O_xO_y\ket{i, b} = (-1)^{b(x_i \oplus y_i)}\ket{i, b}$. Let $P_0 = I \otimes \dyad{0}{0}$ denote the projection onto the $b = 0$ states, and let $P_i$ denote the projection $\dyad{i, 1}{i, 1}$. (As with $O_x$, the projections $P_i$ implicitly act as the identity on any ancilla registers, so $\sum_{i=0}^n P_i = I$.) Then $O_xO_y = P_0 + \sum_{i=1}^n (-1)^{x_i \oplus y_i}P_i$, so $O_xO_y - I = -2\sum_{i: x_i \neq y_i} P_i$. Thus we have:
\begin{equation}
    W^{j+1} - W^j = -2\sum_{x, y \in S}\sum_{i: x_i \neq y_i} \Gamma_{xy}a_x^* a_y \bra{\psi^j_x}P_i\ket{\psi^j_y}
\end{equation}
Now for each $i \in \set{1, \ldots, n}$, let $\Gamma_i$ be a matrix with:
\begin{equation}
    (\Gamma_i)_{xy} \coloneqq \begin{cases}
        \Gamma_{xy} & \text{if $x_i \neq y_i$}
        \\ 0 & \text{if $x_i = y_i$}
    \end{cases}
\end{equation}
Then we have (doing more algebra and the proposition):
\begin{equation}
    \abs{W^{j+1} - W^j} \leq 2\max_{i \in \set{1, \ldots, n}}\norm{\Gamma_i}.
\end{equation}
Since $\abs{\Omega^0} = \norm{\Gamma}$, we have:
\begin{equation}
    \abs{W^t} \geq \norm{\Gamma} - 2t\max_{i \in \set{1, \ldots, n}}\norm{\Gamma_i}
\end{equation}
Thus, we have $\abs{W^t} \leq 2\sqrt{\e(1-\e)}\abs{\Gamma}$, we require:
\begin{equation}
    t \geq \frac{1-2\sqrt{\e(1-e)}}{2}\text{Adv}(f)
\end{equation}
where:
\begin{equation}
    \text{Adv}(f) \coloneqq \max_\Gamma \frac{\norm{\Gamma}}{\max_{i \in \set{1, \ldots, n}}\norm{\Gamma_i}}
\end{equation}
with the maximum taken over all adversary matrices $\Gamma$ for the function $f$. Sometimes the notation $\text{Adv}(f)$ is reserved for the maximization over nonnegative adversary matrices, with $\text{Adv}^\pm(f)$ for the generalized method.

\subsection*{Unstructured search}
As a simple application of this method, we prove the optimality of Grover’s algorithm. It suffices to consider the problem of distinguishing between the case of no marked items and the case of a unique marked item (in an unknown location). Thus, consider the partial function where $S$ consists of strings of Hamming weight $0$ or $1$, and $f$ is the logical OR of the input bits. 

For this problem, adversary matrices take the form:
\begin{equation}
    \Gamma = \m{0 & \gamma_1 & \cdots & \gamma_n \\ \gamma_1 & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ \gamma_n & 0 & \cdots & 0}
\end{equation}
for some nonnegative $\gamma_1, \ldots, \gamma_n$. By symmetry we set them all equal - this can be formalized, but for now let's just consider it as an ansatz.

Setting $\gamma_1 = \ldots = \gamma_n = 1$ (since the overall scale is irrelevant for the bound), we have:
\begin{equation}
    \Gamma^2 = \m{n & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 1 & \cdots & 1}
\end{equation}
which has norm $\norm{\Gamma^2} = n$, and hence $\norm{\Gamma} = \sqrt{n}$. We also have:
\begin{equation}
    \Gamma_1 = \m{0 & 1 & 0 & \cdots & 0 \\ 1 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & \cdots & 0 \\ & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 0}
\end{equation}
and similarly for the other $\Gamma_i$, so $\norm{\Gamma_i} = 1$. Thus, we find that $\text{Adv}(\text{OR}) \geq \sqrt{n}$, and so $Q_{\e}(\text{OR}) \geq \frac{1-2\sqrt{\e(1-\e)}}{2}\sqrt{n}$. This shows that Grover's algorithm is optimal up to a constant factor (recall that Grover finds a unique marked iterm with constant probability in $\frac{\pi}{4}\sqrt{n} + o(1)$ queries).